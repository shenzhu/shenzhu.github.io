---
title: "The Contract-Powered Data Platform"
tags:
  - æ•°æ®ç³»ç»Ÿ
  - ä¸šç•Œå®è·µ
  - Event System
---

è¿™ç¯‡ç¬”è®°æ¥è‡ªäº[Buz Blog](https://buz.dev/blog/the-contract-powered-data-platform)

# 1. Components of a Good Data Platform

## 1.1. Instrumentation and Integration

è¿™æ¡æ¯”è¾ƒä¸è¨€è‡ªæ˜ï¼Œé¦–å…ˆå¿…é¡»æœ‰dataï¼Œä¹‹åæ‰èƒ½è®¨è®ºå®ƒçš„quality

## 1.2. The pipeline

Pipelinesé€šå¸¸æ˜¯`batch`æˆ–è€…`streaming`ï¼Œè™½ç„¶è¿™ä¸¤ç§religionç»å¸¸æœ‰äº‰è®ºï¼Œä½†ä¸€äº›ç›¸ä¼¼çš„conceptæ˜¯å…±é€šçš„. Pipelines collect dat aand put it somewhere

Best pipelinesé€šå¸¸æœ‰ä»¥ä¸‹ç‰¹ç‚¹:
- Move data reliably
- Annotate payloads with metadata such as provenance, `collected_at` timestamps, fingerprints, etc.
- Generate stats to provde the operator with feedback
- Validate and bifurcate payloads, if you're lucky
- Know about and act on payload sensitivities - obfuscate, hash, tokenize, redact, redirect, etc
- Minimize moving pieces
- Don't spend all the CEO's ğŸ’¸ğŸ’¸ğŸ’¸ so they can afford that house in the Bahamas

## 1.3. Storage and access

Storage/access systems range from a wee little Postgres database, to Snowflake, to a data lake filled with Parquet fish and the Loch Ness Trino monster

## 1.4. Date Discovery

As things scale, pipelines/databases/data modelså˜å¾—è¶Šæ¥è¶Šç¹æ‚ï¼Œè¿™æ—¶å€™discoveryå°±éå¸¸é‡è¦

## 1.5. Observability, Monitoring, and Alerting

# 2. Design Goals of a Good Data Platform

## 2.1. Comply with rules

æ¯”å¦‚CPRA

## 2.2. Minimize bad data

æœ‰bad dataæœ¬èº«å·²ç»å¾ˆåäº†ï¼Œæ›´ç³Ÿçš„æ˜¯ä¸çŸ¥é“bad dataå­˜åœ¨

## 2.3. Maximize knowledge of what the system is doing

è¦æœ‰å„ç§å„æ ·çš„monitoring/logsæ¥äº†è§£ç³»ç»Ÿçš„ç°çŠ¶

## 2.4. Minimize friction for all parties involved

å‚ä¸å¼€å‘/ä½¿ç”¨/ç»´æŠ¤data platformçš„engineerå¯èƒ½æ¥è‡ªå„ä¸ªæ–¹é¢ï¼Œç”šè‡³æ˜¯ä¸å¤ªæ‡‚æŠ€æœ¯çš„data engineerï¼Œè¦å°½é‡è®©è¿™äº›äººéƒ½æœ‰good exprience

Want to get buy-in? Minimize friction. Want to increase adoption? Automate others' toil. Want sustainable systems? Reduce cognitive load

# 3. The Contract-Powered Platform

Schemas are the nucleus of sustainable data platforms

åœ¨ä¸€å¼€å§‹å°±enfornce schemaçš„é€‰é¡¹é€šå¸¸å¹¶ä¸è¢«é‡‡çº³å¹¶ä¸”è¢«è®¤ä¸ºæ˜¯unnecessary overheadï¼Œä½†å®ƒæ˜¯ä¸€é¡¹é‡è¦çš„é•¿æœŸæŠ•èµ„ï¼Œè€Œä¸”ä¹‹åå†enfornce schemaä¼šæ¯”è¾ƒéº»çƒ¦

- Schemas empower the "producer" <-> "consumer" relationship
- Schemas are data discovery: Schematizing data upfront means data discovery and documentation writes itself
- Schemas power data validation in transit
- Schemas help stop bad instrumentation from being implemented in the first place
- Schemas improve code quality
- Schemas power automation: Destination tables can be automatically created and migrated as schemas evolve
- Schemas as observability: Calculating namespace-level statistics and splicing them into observability tools is the natural next step, å½“æ¯ä¸ªpayloadéƒ½æœ‰å¯¹åº”çš„namespace/schemaå¹¶ä¸”ä¸ç›¸å…³çš„Datadogç­‰metricsç³»ç»Ÿæ•´åˆèµ·æ¥çš„æ—¶å€™ï¼Œç›¸å…³çš„metrics/tableéƒ½å¯ä»¥å˜å¾—self-serve
- Schemas power compliance-oriented requirements
- Schemas are the foundation of higher-order data models: It is pretty easy to turn a schema into a [dbt source](https://docs.getdbt.com/docs/build/sources) so analytics engineers can reliably build upon a well-defined, trustworthy foundation
- Schemas are the foundation of data products

# 4. The Contract-Powered Workflow

## 4.1. Draft, iterate on, and deploy a schema

Non-engineer stakeholderså¯ä»¥è‡ªå·±å®šä¹‰æœ€åˆç‰ˆæœ¬çš„schemaï¼Œè€Œä¸éœ€è¦æ¥è‡ªdata platform teamçš„äººå‚ä¸

## 4.2. Bring tracking libraries and systems up to parity

æ¯å½“æ–°çš„schemaåŠ å…¥æˆ–è€…versionæ›´æ–°çš„æ—¶å€™ï¼Œautomation kicks in and (at minimum):
- Builds and deploys new tracking SDK's for engineering teams
- Pushes schema metadata âˆ† to data discovery tools
- Ensures infrastructure dependencies (Kafka topics, database tables, etc)
- Pushes the schema to the appropriate place for pipeline-level validation
- Creates dbt sources for the analytics engineers

## 4.3. Implement tracking

ä¸è®ºæ˜¯frontend, backendè¿˜æ˜¯infrastructure toolï¼Œéƒ½å¯ä»¥å‘é€tracking dataåˆ°ç³»ç»Ÿé‡Œ

## 4.4. Deploy

With contract-powered workflows the following prereqs are taken care of before instrumentation rolls out, not after:
- Implementers and stakeholders talk to each other using shared verbiage.
- Versioned, language-specific data structures are generated like all other code dependencies.
- Metadata is pushed to discovery tools.
- The pipeline is primed to accept incoming payloads and mark them as "good" or "bad".
- Observability tools are ready to go for instantaneous feedback in development and production.
- Downstream analytics/modeling entrypoints (like dbt sources) are in place and can be immediately used.

# 5. The Schema-Powered Future

Some other reading if you want to dive in:

- [Data Wrangling at Slack](https://slack.engineering/data-wrangling-at-slack/)
- [Jitney at AirBnb](https://www.slideshare.net/alexismidon/jitney-kafka-at-airbnb)
- [Pegasus at LinkedIn](https://engineering.linkedin.com/blog/2020/pegasus-data-language)
- [Dragon at Uber](https://1fykyq3mdn5r21tpna3wkdyi-wpengine.netdna-ssl.com/wp-content/uploads/2020/03/Schema-Integration-at-Uber-Scale-US2TS-2020-1.pdf)
- [Building Scalable Real Time Event Processing with Kafka and Flink at Doordash](https://doordash.engineering/2022/08/02/building-scalable-real-time-event-processing-with-kafka-and-flink/)
- [Data Mesh at Netflix](https://netflixtechblog.com/data-mesh-a-data-movement-and-processing-platform-netflix-1288bcab2873)
- [Building a Real-time Buyer Signal Data Pipeline for Shopify Inbox](https://shopify.engineering/real-time-buyer-signal-data-pipeline-shopify-inbox)